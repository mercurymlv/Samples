{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Use SQLAlchemy and Pandas to create a connection to Oracle EBS (SBP609) and MMS (JDA/iseries)\n",
    "\n",
    "We will mostly rely on the Pandas package for querying, processing, and file I/O.\n",
    "\n",
    "I'm using pyodbc for MMS as I have not been sucessful in creating a connection string using create_engine. \n",
    "Therefore, add'l users must update their MMS ODBC connection name below or create a new one called \"MMS Prod\" (I think it needs to be in the 64 bit version of ODBC Admin, but if doesn't work, try 32).\n",
    "\n",
    "\n",
    "***Connection: sbp609***\n",
    "\n",
    "***Connection: mms***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import db_connections as db\n",
    "import os\n",
    "import sys\n",
    "from dateutil.parser import parse\n",
    "from time import strftime\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# connection settting - old stuff, import db_connections.py instead\n",
    "# sbp609 = create_engine(\"oracle+cx_oracle://mavaldez:Xdxdxdxd3@SBP609_LX40816_DG.world\", arraysize=5000)\n",
    "# sbp609 = create_engine(\"oracle+cx_oracle://mavaldez:Bhbhbhbh3@SBP609.world\", arraysize=5000)\n",
    "# mms = pyodbc.connect(\"DSN=MMS Prod;UID=mavaldez;PWD=Bhbhbhbh3\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####***Global variables and config settings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "WindowsError",
     "evalue": "[Error 3] The system cannot find the path specified: 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Reports\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-68a222fd0f47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Re-create Reports and Data directories in case they are deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreportspath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreportspath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWindowsError\u001b[0m: [Error 3] The system cannot find the path specified: 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Reports\\\\'"
     ]
    }
   ],
   "source": [
    "# path = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\New Process\\\\MV\\\\'\n",
    "\n",
    "\n",
    "mvnspath = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\'\n",
    "path = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\'\n",
    "reportspath = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Reports\\\\'\n",
    "datapath = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Data\\\\'\n",
    "pipepath = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Pipeline Reports\\\\'\n",
    "uploadpath = 'O:\\\\CoOp\\\\CoOp745_DPM\\\\New Stores\\\\MV Process\\\\MV\\\\Uploads\\\\'\n",
    "# nsfile = 'New_Stores_2018_V2 - mv test minimized.xlsx'\n",
    "nsfile = 'New_Stores_2019_V2.xlsx'\n",
    "\n",
    "### Number of days-out for the \"Missing ODS\" report\n",
    "missingodsdays = 60\n",
    "\n",
    "\n",
    "if not os.path.exists(path):  \n",
    "    print \"Directory missing!!! %s\" % path\n",
    "\n",
    "\n",
    "# Re-create Reports and Data directories in case they are deleted\n",
    "if not os.path.exists(reportspath):\n",
    "    os.mkdir(reportspath)\n",
    "if not os.path.exists(datapath):\n",
    "    os.mkdir(datapath)\n",
    "if not os.path.exists(pipepath):\n",
    "    os.mkdir(pipepath)      \n",
    "if not os.path.exists(uploadpath):\n",
    "    os.mkdir(uploadpath) \n",
    "    \n",
    "\n",
    "print \"Files will be saved to: %s\\n\\nThe file we are using is: %s\" % (mvnspath,nsfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Oracle SQL query to pull in store ***SOURCE GROUP*** assignments in DPM\n",
    "\n",
    "One DF will look for stores that are assigned to a group, the other will be the total sourcing rule table\n",
    "\n",
    "***DF Name: src***: Store assignments\n",
    "\n",
    "***DF Name: src_rules***: Active sourcing rules in DPM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = pd.read_sql_query(\"\"\"select distinct cast(xdps.STORE_NUMBER as int) as \"str\",\n",
    "    cast(xdps.CUSTOMER_NUMBER as int) as \"custnum\",\n",
    "    cast(xdps.PARTY_SITE_NUMBER as int) as \"site\",\n",
    "    xdps.SOURCING_GROUP_CODE as \"srcgrp\"\n",
    "    \n",
    "    from APPS.XXOM_DSTRB_POINT_STORES_V xdps\n",
    "    where 1=1\n",
    "    --and xdps.STORE_NUMBER in (48855,101,23166,48923)\n",
    "    and (xdps.END_DATE > trunc(sysdate) or xdps.END_DATE is null)\n",
    "    and xdps.STORE_NUMBER is not null\"\"\", db.sbp609)\n",
    "\n",
    "\n",
    "src_rules = pd.read_sql_query(\"\"\"select xdpsr.STORE_GROUP \"srcgrp\", xdpsr.SOURCING_PRECEDENCE \"prec\" , xdpsr.SOURCE_TYPE \"dptype\", xdpsr.DSTRB_POINT_NUMBER \"dpnum\" , xdpsr.DP_NAME \"dpname\" \n",
    "\n",
    "from APPS.XXOM_DP_SOURCING_RULES_V xdpsr\n",
    "\n",
    "where 1=1\n",
    "\n",
    "and (xdpsr.END_DATE >= trunc(sysdate) or xdpsr.END_DATE is null)\"\"\", db.sbp609)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Oracle SQL query to pull in ***STORE NUMBERS*** from Oracle customer tables\n",
    "\n",
    "Returns stores with an active store number, customer account, and primary ship-to site set up in Oracle\n",
    "\n",
    "We will use this to determine when we can upload sourcing into DPM.\n",
    "\n",
    "***DF Name: stroracle***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroracle = pd.read_sql_query(\"\"\"select distinct cast(hp.ATTRIBUTE21 as int) as \"pty_strnum\"\n",
    "from apps.hz_parties hp\n",
    "join apps.hz_party_sites hps on hp.PARTY_ID=hps.PARTY_ID\n",
    "join apps.hz_cust_acct_sites_all hcas on hps.PARTY_SITE_ID=hcas.PARTY_SITE_ID\n",
    "join apps.hz_cust_site_uses_all hcsu on hcas.CUST_ACCT_SITE_ID= hcsu.CUST_ACCT_SITE_ID\n",
    "join apps.hz_cust_accounts hca on hcas.CUST_ACCOUNT_ID=hca.CUST_ACCOUNT_ID\n",
    "\n",
    "where 1=1\n",
    "\n",
    "and hp.ATTRIBUTE21 is not null\n",
    "and hcsu.SITE_USE_CODE='SHIP_TO'\n",
    "and hcsu.PRIMARY_FLAG = 'Y'\n",
    "and hca.STATUS = 'A'\n",
    "and hcas.STATUS = 'A'\"\"\", db.sbp609)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Oracle SQL query to pull in store ***ODS*** assignments in DPM\n",
    "\n",
    "Looks for the active Standard ODS effective of the date the query is run\n",
    "\n",
    "Will return 'blank' schedules (which can be identified as 0 freq)\n",
    "\n",
    "\n",
    "***DF Name: ods***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ods = pd.read_sql_query(\"\"\"select distinct cast(xcs.STORE_NUMBER as int) as \"strnum\",\n",
    "    cast(xcs.CUSTOMER_NUMBER as int) as \"cstnum\",\n",
    "    cast(xcs.SITE_NUMBER as int) as \"sitenum\",\n",
    "    xdpm.DP_NAME as \"DPName\",\n",
    "    xdpm.DSTRB_POINT_NUMBER as \"DPNum\",\n",
    "    \n",
    "    nvl2(xcs.CUST_SCHED_ID,(DECODE (mon_week_1,'Y',1,0)+ DECODE (tue_week_1,'Y',1,0)+ DECODE (wed_week_1,'Y',1,0)+ DECODE (thu_week_1,'Y',1,0)+ DECODE (fri_week_1,'Y',1,0)+ DECODE (sat_week_1,'Y',1,0)+ DECODE (sun_week_1,'Y',1,0)+ DECODE (mon_week_2,'Y',1,0)+ DECODE \n",
    "(tue_week_2,'Y',1,0)+ DECODE (wed_week_2,'Y',1,0)+ DECODE (thu_week_2,'Y',1,0)+ DECODE (fri_week_2,'Y',1,0)+ DECODE \n",
    "(sat_week_2,'Y',1,0)+ DECODE (sun_week_2,'Y',1,0) )/2, xcs.CUST_SCHED_ID) \"Freq\"\n",
    "\n",
    "    from APPS.XXOM_CUSTOMER_SCHEDULE_MNT_V xcs\n",
    "    join APPS.XXOM_DSTRB_POINT_MSTR xdpm on xcs.DSTRB_POINT_ID=xdpm.DSTRB_POINT_ID\n",
    "    where 1=1 and xcs.STORE_NUMBER is not null\n",
    "    and xcs.SCHEDULE_TYPE='Standard'\n",
    "    and xcs.IN_STORE_DEL_SCH_START_DATE <= trunc(sysdate)\n",
    "    and (xcs.IN_STORE_DEL_SCH_END_DATE > trunc(sysdate) or xcs.IN_STORE_DEL_SCH_END_DATE is null)\"\"\", db.sbp609)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####MMS Query to build an ***MMS ACTIVE STORE LIST*** \n",
    "\n",
    "This query returns a large number of records ~25000\n",
    "\n",
    "This is a complete list of stores in NA Legal Entities - not just 'new stores' since we may want\n",
    "to access address and other data for recent stores\n",
    "\n",
    "The 'proposed open date' needs quite a bit of manipulation to turn into a useable date-format - \n",
    "including a 'case' statement to handle nulls (0 in MMS). Converting the date in SQL appears to take a long time to run. Better to include the \"Century\" field and just write a Python function to do the converions to a proper date format.\n",
    "\n",
    "***DF Name: mms_str***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# start_time=time.time()\n",
    "\n",
    "def ProposedOpen(cen,proposed):\n",
    "    from datetime import date\n",
    "    if proposed == 0:\n",
    "        return pd.NaT\n",
    "    year = 1900 + (100 * cen) + np.floor(proposed/10000)\n",
    "    month = np.floor(np.mod(proposed,10000)/100)\n",
    "    day = np.floor(np.mod(proposed,100))\n",
    "    pdate = date(int(year),int(month),int(day))\n",
    "    return pdate\n",
    "\n",
    "\n",
    "mms_str = pd.read_sql_query(\"\"\"select cast(tbl.STRNUM as int) as \"strnum\" , tbl.STRNAM,\n",
    "sadd.ADDRESS_LINE_1, sadd.ADDRESS_LINE_2, sadd.CITY,\n",
    "sadd.COUNTRY_SUBDIVISION_CODE as \"State/Prov\", sadd.POSTAL_CODE, sadd.COUNTRY_CODE,\n",
    "\n",
    "--case sopt.STPODT when 0 then date(0) else date((1900+sopt.STPOCN*100+floor(sopt.STPODT/10000)) || '-' ||floor(mod(sopt.STPODT,10000)/100) || '-' || floor(mod(sopt.STPODT,100))) \n",
    "--end as \"Proposed Open Date\",\n",
    "\n",
    "cast(sopt.STPOCN as int) as \"open_century\",\n",
    "\n",
    "cast(sopt.STPODT as int) as \"open_proposed\",\n",
    "\n",
    "sadd.LATITUDE, sadd.LONGITUDE, tbl.STCOMP \"Legal Entity\",\n",
    "tbl2.OWNERSHIP_TYPE, fmt.SIZE_CODE\n",
    "\n",
    "from MM4R3LIB.TBLSTR tbl\n",
    "join MM4R3LIB.STROPT sopt on tbl.STRNUM= sopt.STLOC\n",
    "join MM4R3LIB.TBLSTR2 tbl2 on tbl.STRNUM= tbl2.STORE_NUMBER\n",
    "join MM4R3LIB.STRADR sadd on tbl.STRNUM= sadd.STORE_NUMBER and sadd.ADDRESS_TYPE='P' and sadd.LANGUAGE_ID='ENG'\n",
    "join MM4R3LIB.STRFMT fmt on tbl.STRNUM= fmt.STORE_NUMBER\n",
    "where 1=1\n",
    "--and tbl.STRNUM in (101,49985,4642,6628,364)\n",
    "and tbl.STCOMP in (100,128,142,150,152,160,302)\n",
    "and tbl2.PURPOSE_TYPE_NAME='STORE' and tbl.STCLDT = 0\"\"\", db.mms)\n",
    "\n",
    "mms_str['proposed_open_date'] = mms_str.apply(lambda x: ProposedOpen(x['open_century'],x['open_proposed']),axis=1)\n",
    "mms_str['proposed_open_date'] = pd.to_datetime(mms_str['proposed_open_date'],errors='coerce')\n",
    "mms_str['proposed_open_date'] = mms_str['proposed_open_date'].dt.date\n",
    "mms_str.drop(columns=['open_century', 'open_proposed'], inplace=True)\n",
    "\n",
    "### Not sure why, but these fields come out with a bunch of white space at end\n",
    "mms_str['STRNAM'] = mms_str['STRNAM'].apply(lambda x: x.strip())\n",
    "mms_str['ADDRESS_LINE_1'] = mms_str['ADDRESS_LINE_1'].apply(lambda x: x.strip())\n",
    "mms_str['ADDRESS_LINE_2'] = mms_str['ADDRESS_LINE_2'].apply(lambda x: x.strip())\n",
    "mms_str['State/Prov'] = mms_str['State/Prov'].apply(lambda x: x.strip())\n",
    "mms_str['CITY'] = mms_str['CITY'].apply(lambda x: x.strip())\n",
    "mms_str['POSTAL_CODE'] = mms_str['POSTAL_CODE'].apply(lambda x: x.strip())\n",
    "mms_str['SIZE_CODE'] = mms_str['SIZE_CODE'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####MMS Query to build current ***MMS LIFECYCLES***\n",
    "\n",
    "***DF Name: str_life***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time=time.time()\n",
    "\n",
    "str_life = pd.read_sql_query(\"\"\"Select cast(stran.STORE_NUMBER as int) \"strnum\", stt.TRANSITION_DESCRIPTION, to_char(stran.TRANSITION_START_DATE,'YYYY-MM-DD') \"TRANSITION_START_DATE\", to_char(stran.TRANSITION_END_DATE,'YYYY-MM-DD') \"TRANSITION_END_DATE\",\n",
    "\n",
    "--case stran.TRANSITION_END_DATE when '0001-01-01' then date(0) else stran.TRANSITION_END_DATE end \"TRANSITION_END_DATE\",\n",
    "tbltran.STARTING_PHASE , tbltran.ENDING_PHASE , tbltran.OPEN_FOR_BUSINESS\n",
    "\n",
    "from MM4R3LIB.STRTRN stran\n",
    "join MM4R3LIB.TBLTRN tbltran on stran.TRANSITION_ID= tbltran.TRANSITION_ID\n",
    "join MM4R3LIB.TBLTRNTL stt on stran.TRANSITION_ID=stt.TRANSITION_ID and stt.LANGUAGE_ID='ENG'\n",
    "\n",
    "where 1=1\n",
    "--and stran.STORE_NUMBER in (101,104)\n",
    "and stran.TRANSITION_START_DATE <= curdate()\n",
    "and (stran.TRANSITION_END_DATE >= curdate() or stran.TRANSITION_END_DATE='0001-01-01')\"\"\", db.mms)\n",
    "\n",
    "### Convert the weird MMS \"blank\" format to an actual blank instead\n",
    "str_life['TRANSITION_START_DATE'].replace('0001-01-01','',inplace=True)\n",
    "str_life['TRANSITION_END_DATE'].replace('0001-01-01','',inplace=True)\n",
    "\n",
    "### Not sure why, but these fields come out with a bunch of white space at end\n",
    "str_life['TRANSITION_DESCRIPTION'] = str_life['TRANSITION_DESCRIPTION'].apply(lambda x: x.strip())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####MMS - DPM ***Exclusion list*** from Location Mater\n",
    "\n",
    "***DF Name: lm_exclu***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_exclu = pd.read_sql_query(\"\"\"select cast(exl.STORE_NUMBER as int) \"str\" from MM4R3LIB.STRDPMEX exl\"\"\", mms)\n",
    "\n",
    "lm_exclu = pd.read_sql_query(\"\"\"select cast(exl.STORE_NUMBER as int) \"strx\" from MM4R3LIB.STRDPMEX exl\"\"\", db.mms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Import ***Pipeline Report*** as a flat file\n",
    "\n",
    "***DF Name: pipe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline report is send via email\n",
    "#Save the flat file to the O: drive and import\n",
    "\n",
    "#The report has a timestamp, so get complete list of files in directory\n",
    "#sort descending - based on file name, it will sort on timestamp yyyymmdd\n",
    "filelist = sorted(os.listdir(pipepath), reverse=True)\n",
    "\n",
    "#Null list that will be populated with valid filenames\n",
    "cleanfile = []\n",
    "\n",
    "#Iterate the list of files in the directory for the correct name, build list in \"cleanfile\"\n",
    "for word in filelist:\n",
    "    if word[:16] == 'Pipeline Report_':\n",
    "        cleanfile.append(word)\n",
    "\n",
    "#import the file using the first file name (was sorted)\n",
    "pipe = pd.read_excel('%s%s' % (pipepath,cleanfile[0]))\n",
    "\n",
    "#just want these 4 columns\n",
    "pipe = pipe[['Store Number','CPN','Project Status','Project Type']]\n",
    "\n",
    "#further filter the data\n",
    "pipe = pipe[(pipe['Project Type'] == 'NEW STORE') & (pipe['Project Status'] <> 'Retired') & (pipe['Project Status'] <> 'On-Hold')]\n",
    "pipe = pipe.dropna(subset=['Store Number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cleanfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Use Pandas to read the current ***'NEW STORE' FILE*** on the O: drive.\n",
    "\n",
    "Update the 'new store' file with the most current data (from SQL statements above), this includes (in order):\n",
    "\n",
    "    Exclusion List Status\n",
    "    Active Oracle Account Check\n",
    "    DPM Sourcing\n",
    "    ODS Check\n",
    "    Full Address (from MMS)\n",
    "    Proposed Open Date\n",
    "    Proposed Open Date: Delta\n",
    "    Store Dev Project Info\n",
    "    \n",
    "Export the updated file back to the O: drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ns = pd.read_excel('%s%s' % (path,nsfile),sheet_name=\"New Stores\", skiprows=1)\n",
    "# ns.drop(columns=['Order By','Ship Day','Order By.1','Ship Day.1','Order By.2','Ship Day.2','Order By.3','Ship Day.3','Order By.4','Ship Day.4','Order By.5','Ship Day.5','Order By.6','Ship Day.6','Oracle Account?','Trans ODS Provided','Store Dev Status','Source Match','Dupe Check','Days to Open','Vlookup.1','Unnamed: 53','COUNT'], inplace=True)\n",
    "\n",
    "# # Save just in case...\n",
    "# orig_cols = np.array([ns.columns],dtype=\"string\").copy()\n",
    "\n",
    "###Sometimes spaces get added to the end of column names - clean these up\n",
    "ns.columns = [c.strip() for c in ns.columns.values.tolist()]\n",
    "\n",
    "### Only need to keep these columns\n",
    "ns = ns[['Store Number','Proposed Open Date','DP Name','Remarks','Sourcing Group (Proposed)','X-dock CDC  (Proposed)','X-Dock CDC Receipt Day','X-Dock In store','Additional ITLT','Delivery Window Start','Delivery Window End','Bakery Temperature (Proposed)','Carrier','Terminal','Ship','Delivery','Status']]\n",
    "\n",
    "\n",
    "# Update the proposed open date to the correct format and removed any 'pushed' notes\n",
    "ns['Proposed Open Date'] = pd.to_datetime(ns['Proposed Open Date'],errors='coerce')\n",
    "ns['Proposed Open Date']=ns['Proposed Open Date'].dt.date\n",
    "\n",
    "# Flag if the store is still on the Location Master Exclusion list\n",
    "ns = pd.merge(ns, lm_exclu, left_on='Store Number', right_on='strx', how='left')\n",
    "ns.rename(columns={'strx':'On Exclusion List?'}, inplace=True)\n",
    "#change non-null values to \"Y\" so that it is more readable/understandable\n",
    "ns['On Exclusion List?'].where(ns['On Exclusion List?'].isnull(),'Y',inplace=True)\n",
    "\n",
    "\n",
    "### Check if store has an active account in Oracle\n",
    "ns = pd.merge(ns, stroracle, left_on='Store Number', right_on='pty_strnum', how='left')\n",
    "ns.rename(columns={'pty_strnum':'Oracle Account Active?'}, inplace=True)\n",
    "#change non-null values to \"Y\" so that it is more readable/understandable\n",
    "ns['Oracle Account Active?'].where(ns['Oracle Account Active?'].isnull(),'Y',inplace=True)\n",
    "\n",
    "\n",
    "###Update Customer Number and Site Number based on DPM sourcing \n",
    "ns = pd.merge(ns, src, left_on='Store Number', right_on='str', how='left')\n",
    "ns.drop(columns=['str'], inplace=True)\n",
    "ns.rename(columns={'custnum':'Customer Number', 'site':'Customer Site Number','srcgrp':'DPM Sourcing'},inplace=True)\n",
    "\n",
    "\n",
    "###Check if ODS is loaded for store/DP combination\n",
    "ns = pd.merge(ns, ods, left_on=['Store Number','DP Name'], right_on=['strnum','DPName'], how='left')\n",
    "ns.drop(columns=['strnum','cstnum','sitenum','DPNum','DPName'], inplace=True)\n",
    "ns.rename(columns={'Freq':'DPM ODS/Freq'},inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "###Update Address and Proposed Open Date from MMS\n",
    "ns = pd.merge(ns, mms_str, left_on='Store Number', right_on='strnum', how='left')\n",
    "\n",
    "###Check for change in proposed open-date\n",
    "# save the delta to output to a report later\n",
    "ns['Proposed Open Delta'] = (ns['proposed_open_date'] - ns['Proposed Open Date']).dt.days\n",
    "pdelta = ns[['Store Number','Proposed Open Date','proposed_open_date','Proposed Open Delta']].copy()\n",
    "pdelta.rename(columns={'Proposed Open Date':'Old Proposed','proposed_open_date':'New Proposed'}, inplace=True)\n",
    "\n",
    "\n",
    "###Clean up column names\n",
    "ns.drop(columns=['Proposed Open Date','LATITUDE','LONGITUDE','strnum'], inplace=True)\n",
    "ns.rename(columns={'STRNAM':'Store Name','ADDRESS_LINE_1':'Ship-to Address 1','ADDRESS_LINE_2':'Address 2',\n",
    "                       'CITY':'City','State/Prov':'State/Province','POSTAL_CODE':'Zip',\n",
    "                       'COUNTRY_CODE':'Country','OWNERSHIP_TYPE':'BU','proposed_open_date':'Proposed Open Date'},inplace=True)\n",
    "\n",
    "\n",
    "ns = pd.merge(ns, pipe, on='Store Number', how='left')\n",
    "ns.drop(columns=['CPN','Project Type'], inplace=True)\n",
    "\n",
    "# output_columns = ('Store Number','Customer Number','Customer Site Number','Store Name','Ship-to Address 1','Address 2','City','State/Province','Zip','Country','BU','Legal Entity','Proposed Open Date','Proposed Open Delta','Project Status','DP Name','Remarks','Sourcing Group (Proposed)','X-dock CDC  (Proposed)','X-Dock CDC Receipt Day','X-Dock In store','Additional ITLT','Delivery Window Start','Delivery Window End','Bakery Temperature (Proposed)','SIZE_CODE','Carrier','Terminal','Ship','Delivery','Status','On Exclusion List?','Oracle Account Active?','DPM Sourcing','DPM ODS/Freq')\n",
    "\n",
    "ns = ns[['Store Number','Customer Number','Customer Site Number','Store Name','Ship-to Address 1','Address 2','City','State/Province','Zip','Country','BU','Legal Entity','Proposed Open Date','Proposed Open Delta','Project Status','DP Name','Remarks','Sourcing Group (Proposed)','X-dock CDC  (Proposed)','X-Dock CDC Receipt Day','X-Dock In store','Additional ITLT','Delivery Window Start','Delivery Window End','Bakery Temperature (Proposed)','SIZE_CODE','Carrier','Terminal','Ship','Delivery','Status','On Exclusion List?','Oracle Account Active?','DPM Sourcing','DPM ODS/Freq']].copy()\n",
    "\n",
    "ns.sort_values(['Proposed Open Date','Store Number'])\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter('%s\\\\New Store Tracker_{}.xlsx'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) % mvnspath)\n",
    "ns.to_excel(writer,sheet_name='New Stores',columns=None, index=False)\n",
    "\n",
    "writer.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Check for ***RDC ODS***, format, and produce an upload file\n",
    "\n",
    "Check the columns where Transportation provides the \"ship\" and \"delivery\" convert into the proper format (for ODS and windows) and output to O: Drive. Produce 3 files:\n",
    "\n",
    "ODS upload\n",
    "\n",
    "MCL upload\n",
    "\n",
    "Errors (bad data that fails validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to check for misspellings and, in general, make days uniform\n",
    "def clean_days(day):\n",
    "    if day in ['MON','TUE','WED','THU','FRI','SAT','SUN']:\n",
    "        return day\n",
    "    elif day in ['M','MN','MONS','MOND','MONDAY','MONSDAY']:\n",
    "        return 'MON'\n",
    "    elif day in ['T','TUES','TUESDAY','TUEDAY']:\n",
    "        return 'TUE'\n",
    "    elif day in ['W','WEDS','WEN','WENS','WEDNESDAY','WEDDAY','WENSDAY','WEDNDAY']:\n",
    "        return 'WED'\n",
    "    elif day in ['R','H','THUR','THURS','THUS','THURSDAY','THURDAY']:\n",
    "        return 'THU'\n",
    "    elif day in ['F','FRIS','FRID','FRIDAY','FR']:\n",
    "        return 'FRI'\n",
    "    elif day in ['S','SATS','SATURDAY','SATERDAY','SATSURDAY','SADURDAY']:\n",
    "        return 'SAT'\n",
    "    elif day in ['U','SUNS','SUND','SUNDAY','SUNSDAY','SUDDAY']:\n",
    "        return 'SUN'\n",
    "    else:\n",
    "        return 'Invalid Day'\n",
    "\n",
    "    \n",
    "### Log any errors with RDC to export as report, check for:\n",
    "###  * Days (Ship or Delivery) are not valid names\n",
    "###  * Ship and Delivery are the same - i.e. check ITLT\n",
    "###  * Invalid DP Name\n",
    "###  * Invalid windows\n",
    "def ods_errors (ship,deliv,name,state,start,end):\n",
    "    ### Create empty string to hold errors, append multiple errors as you go\n",
    "    errors = str()\n",
    "    \n",
    "    ### These are states/provinces that may have ITLT >= 7 - flag as warning to check ITLT\n",
    "    llstates = ('AK','HI','NL','ND','SD','WY','YT')\n",
    "    \n",
    "    if ship == 'Invalid Day':\n",
    "        errors += 'Invalid Ship; '\n",
    "    if deliv == 'Invalid Day':\n",
    "        errors += 'Invalid Delivery; '\n",
    "    if ship == deliv:\n",
    "        errors += 'Ship and Delivery Same Day; '\n",
    "    if state in llstates:\n",
    "        errors += 'Check ITLT - %s; ' % state    \n",
    "    if name is False:\n",
    "        errors += 'Invalid DP Name; '\n",
    "    if (start == 'Invalid Time') | (end == 'Invalid Time'):\n",
    "        errors += 'Invalid Window'\n",
    "       \n",
    "    return errors\n",
    "\n",
    "\n",
    "### Function to parse windows and return in 24hr format, e.g. 22:00, etc...\n",
    "def clean_windows(ctime):\n",
    "    ### If windows not submitted, prompt use to just fill in the default window (i.e. 10:00 to 14:00)\n",
    "    if ctime == \"NAN\":\n",
    "        return \"Use Default\"\n",
    "    ### Check for different formats of midnight and noon\n",
    "    elif (ctime == \"0\") | (ctime.upper() == \"MIDNIGHT\") | (ctime == \"24:00\"):\n",
    "        return \"0:00\"\n",
    "    elif ctime.upper() == 'NOON':\n",
    "        return \"12:00\"\n",
    "    ### Convert when in integer format, e.g, when 2300 is 23:00\n",
    "    elif ctime.isdigit():\n",
    "        ### In this format, should be a lengenth of 3 or 4 only, if 4, should not be bigger than 23\n",
    "        if not(2 < len(ctime) < 5):\n",
    "            return \"Invalid Time\"\n",
    "        elif int(ctime[:len(ctime)-2]) > 23:\n",
    "            return \"Invalid Time\"\n",
    "        else:\n",
    "            return ctime[:len(ctime)-2] + \":\" + ctime[-2:]\n",
    "    ### Convert any AM/PM formats to 24hr\n",
    "    elif (\"PM\" in ctime) | ('AM' in ctime):\n",
    "        if ctime[-3:] != \" \":\n",
    "            ctime = ctime[:-2] + \" \" + ctime[-2:] \n",
    "        ctime = time.strptime(str(ctime),\"%I:%M %p\")\n",
    "        return str(time.strftime(\"%H:%M\",ctime))\n",
    "    ### Brief check to make sure it's in some sort of time format\n",
    "    elif \":\" not in ctime:\n",
    "        return \"Invalid Time\"\n",
    "    else:\n",
    "        return ctime\n",
    "    \n",
    "\n",
    "### Dictionaries for mapping order days and ship days\n",
    "weekdays = {np.nan:np.nan, 'MON':'Monday','TUE':'Tuesday','WED':'Wednesday','THU':'Thursday','FRI':'Friday', \\\n",
    "            'SAT':'Saturday','SUN':'Sunday'}\n",
    "orddays = {np.nan:np.nan, 'Tuesday':'Monday','Wednesday':'Tuesday','Thursday':'Wednesday','Friday':'Thursday',\\\n",
    "           'Monday':'Friday','Saturday':'Invalid Ship','Sunday':'Invalid Ship'}\n",
    "\n",
    "\n",
    "### Copy the RDC ODS columns into their own dataframe\n",
    "rdc_ods = ns[['Store Number','DP Name', 'Sourcing Group (Proposed)','State/Province', 'Delivery Window Start','Delivery Window End', \\\n",
    "             'Ship','Delivery','Status','DPM Sourcing','DPM ODS/Freq','Oracle Account Active?']].copy()\n",
    "\n",
    "\n",
    "# Remove rows where ODS is already loaded\n",
    "rdc_ods = rdc_ods.where((rdc_ods['DPM ODS/Freq'].isnull()) | (rdc_ods['DPM ODS/Freq'] == 0))\n",
    "rdc_ods = rdc_ods.dropna(how='all')\n",
    "\n",
    "\n",
    "### Remove any row that does not have ODS filled out and does not have an active Oracle account\n",
    "rdc_ods.dropna(subset=['Ship','Delivery','Oracle Account Active?'], inplace=True)\n",
    "\n",
    "\n",
    "### Skip all the processing steps if there is no records to upload\n",
    "if not rdc_ods.empty:\n",
    "    ### Strip out any extra whitespace, etc...\n",
    "    rdc_ods['Ship'] = map(lambda x:x.strip(),rdc_ods['Ship'])\n",
    "    rdc_ods['Delivery'] = map(lambda x:x.strip(),rdc_ods['Delivery'])\n",
    "    rdc_ods['State/Province'] = map(lambda x:x.strip(),rdc_ods['State/Province'])\n",
    "\n",
    "    ### Convert ship/delivery days,state/prov and windows to uppercase just for uniformity\n",
    "    rdc_ods['Ship'] = map(lambda x:x.upper(),rdc_ods['Ship'])\n",
    "    rdc_ods['Delivery'] = map(lambda x:x.upper(),rdc_ods['Delivery'])\n",
    "    rdc_ods['State/Province'] = map(lambda x:x.upper(),rdc_ods['State/Province'])\n",
    "    ### for the windows, this is to e.g. convert 5:00am to 5:00AM\n",
    "    rdc_ods['parse start'] = rdc_ods['Delivery Window Start'].apply(lambda x: str(x).upper())\n",
    "    rdc_ods['parse end'] = rdc_ods['Delivery Window End'].apply(lambda x: str(x).upper())\n",
    "\n",
    "\n",
    "    ### Clean up the days so that they are in DDD format like MON, etc...\n",
    "    rdc_ods['Clean Ship'] = rdc_ods['Ship'].apply(lambda x: clean_days(x))\n",
    "    rdc_ods['Clean Delivery'] = rdc_ods['Delivery'].apply(lambda x: clean_days(x))\n",
    "\n",
    "    ### Clean up the windows a bit and format in 24hr e.g. 17:00, etc...\n",
    "    rdc_ods['parse start'] = rdc_ods['parse start'].apply(lambda x: clean_windows(str(x)))\n",
    "    rdc_ods['parse end'] = rdc_ods['parse end'].apply(lambda x: clean_windows(str(x)))\n",
    "\n",
    "\n",
    "    ### Optional - add the default 10:00 to 14:00 window if left blank\n",
    "    ### Add logic to make sure both start and end are blank, not just one\n",
    "    rdc_ods['parse start'] = rdc_ods['parse start'].str.replace('Use Default','10:00')\n",
    "    rdc_ods['parse end'] = rdc_ods['parse end'].str.replace('Use Default','14:00')\n",
    "\n",
    "    ### You can use these to format as time if you want, but exports to csv anyway\n",
    "    ### if you use, add logic to exclude or convert \"Invalid Time\" so that it does not raise an error\n",
    "    # rdc_ods['Clean Start'] = rdc_ods['Delivery Window Start'].apply(lambda x: time.strptime(str(x),\"%H%M\"))\n",
    "    # rdc_ods['Clean Start'] = rdc_ods['Clean Start'].apply(lambda x: strftime(\"%H:%M\",x))\n",
    "\n",
    "    # rdc_ods['Clean End'] = rdc_ods['Delivery Window End'].apply(lambda x: time.strptime(str(x),\"%H%M\"))\n",
    "    # rdc_ods['Clean End'] = rdc_ods['Clean End'].apply(lambda x: strftime(\"%H:%M\",x))\n",
    "\n",
    "\n",
    "    ### Build a list of valid DP names and DP Numbers\n",
    "    dps = src_rules[['dpname','dpnum']].copy()\n",
    "    dps.drop_duplicates(inplace=True)\n",
    "    ### Create a set of all DPM DP names to validate against\n",
    "    check_dpname = set(dps['dpname'])\n",
    "\n",
    "    ### Merge so that we can pull in DP number for MCL upload\n",
    "    rdc_ods = pd.merge(rdc_ods, dps, left_on=\"DP Name\", right_on=\"dpname\", how='left')\n",
    "\n",
    "    ### Check if the entered DP Name is a valid DP\n",
    "    rdc_ods['Check Name']=rdc_ods['DP Name'].apply(lambda x: x in check_dpname)\n",
    "\n",
    "\n",
    "    ## Log ODS errors, check for:\n",
    "    ##  * Invalid ship/delivery days\n",
    "    ##  * Invalid DP Name\n",
    "    ##  * Invalid windows \n",
    "\n",
    "    rdc_ods['Errors'] = rdc_ods.apply(lambda x: ods_errors(x['Clean Ship'],x['Clean Delivery'],x['Check Name'],x['State/Province'],x['parse start'],x['parse end']),axis=1)\n",
    "    rdc_errors = rdc_ods[['Store Number','DP Name','Ship','Delivery','Status','Errors']].copy()\n",
    "    rdc_errors = rdc_errors[(rdc_errors['Errors'] != '')]\n",
    "    ### Delete the lines with errors\n",
    "    rdc_ods = rdc_ods[(rdc_ods['Errors'] == '')]\n",
    "\n",
    "\n",
    "    ### Update the days to ODS format - i.e. Monday instead of Mon\n",
    "    ### use a dictionary to map the order day from ship day\n",
    "    rdc_ods['Clean Ship'] = rdc_ods['Clean Ship'].map(weekdays)\n",
    "    rdc_ods['Clean Order'] = rdc_ods['Clean Ship'].map(orddays)\n",
    "\n",
    "\n",
    "    ### Fill in the default header info\n",
    "    rdc_ods['Add/Change/Delete']  = 'Add'\n",
    "    rdc_ods['Copy Flag']  = 'Y'\n",
    "    rdc_ods['Schedule Type']  = 'Standard'\n",
    "\n",
    "\n",
    "    ### Fill out Start and End dates. Start on Monday date of current week; end-date is always null\n",
    "    rdc_ods['Start']  = dt.date.today() - dt.timedelta(days=dt.date.weekday(dt.date.today()))\n",
    "    rdc_ods['End'] = np.nan\n",
    "\n",
    "\n",
    "    ### Create the Order/Ship columns to fit the ODS upload template\n",
    "    rdc_ods['Mon_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'MON')\n",
    "    rdc_ods['Mon_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'MON')\n",
    "\n",
    "    rdc_ods['Tue_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'TUE')\n",
    "    rdc_ods['Tue_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'TUE')\n",
    "\n",
    "    rdc_ods['Wed_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'WED')\n",
    "    rdc_ods['Wed_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'WED')\n",
    "\n",
    "    rdc_ods['Thu_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'THU')\n",
    "    rdc_ods['Thu_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'THU')\n",
    "\n",
    "    rdc_ods['Fri_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'FRI')\n",
    "    rdc_ods['Fri_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'FRI')\n",
    "\n",
    "    rdc_ods['Sat_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'SAT')\n",
    "    rdc_ods['Sat_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'SAT')\n",
    "\n",
    "    rdc_ods['Sun_Ord'] = rdc_ods['Clean Order'].where(rdc_ods['Clean Delivery'] == 'SUN')\n",
    "    rdc_ods['Sun_Ship'] = rdc_ods['Clean Ship'].where(rdc_ods['Clean Delivery'] == 'SUN')\n",
    "\n",
    "    ### Create a new datafrome for ODS upload with just the appropriate columns and order\n",
    "    rdc_output = rdc_ods[['Store Number','Sourcing Group (Proposed)','DPM Sourcing','State/Province','Oracle Account Active?','Status', 'Delivery Window Start','Delivery Window End','DP Name','Add/Change/Delete','Copy Flag','Schedule Type','Start','End', \\\n",
    "                         'Mon_Ord','Mon_Ship','Tue_Ord','Tue_Ship','Wed_Ord','Wed_Ship','Thu_Ord','Thu_Ship', \\\n",
    "                         'Fri_Ord', 'Fri_Ship','Sat_Ord','Sat_Ship','Sun_Ord','Sun_Ship']].copy()\n",
    "\n",
    "    ### Create a new datafrome for MCL upload with just the appropriate columns and order\n",
    "    mcl_output = rdc_ods[['dpnum','Store Number', 'DP Name','Delivery Window Start','Delivery Window End','Sourcing Group (Proposed)','DPM Sourcing', \\\n",
    "                          'State/Province','Oracle Account Active?','Status','parse start','parse end']].copy()\n",
    "    \n",
    "    mcl_output.rename(columns={'dpnum':'DP Number','Delivery Window Start':'Start - submitted',\\\n",
    "                               'Delivery Window End':'End - submitted','parse start':'Delivery Start Time','parse end':'Delivery End Time'}, inplace=True)\n",
    "    mcl_output['Upload Type'] = 'Change'\n",
    "    mcl_output = mcl_output[['Upload Type','DP Number','Store Number','DP Name','Start - submitted','End - submitted',\\\n",
    "                             'Sourcing Group (Proposed)','DPM Sourcing','State/Province','Oracle Account Active?','Status',\\\n",
    "                             'Delivery Start Time','Delivery End Time']]\n",
    "### If there are no records to upload, then just return empty dataframes\n",
    "### We still want the files to output (and be blank) just to prove the program has run\n",
    "else:\n",
    "        rdc_output = pd.DataFrame(columns=('Store Number','Sourcing Group (Proposed)','DPM Sourcing','State/Province','Oracle Account Active?','Status', 'Delivery Window Start','Delivery Window End','DP Name','Add/Change/Delete','Copy Flag','Schedule Type','Start','End', \\\n",
    "                         'Mon_Ord','Mon_Ship','Tue_Ord','Tue_Ship','Wed_Ord','Wed_Ship','Thu_Ord','Thu_Ship', \\\n",
    "                         'Fri_Ord', 'Fri_Ship','Sat_Ord','Sat_Ship','Sun_Ord','Sun_Ship'))\n",
    "        mcl_output = pd.DataFrame(columns=('Upload Type','DP Number','Store Number','DP Name','Start - submitted','End - submitted','Sourcing Group (Proposed)','DPM Sourcing','State/Province','Oracle Account Active?','Status','Delivery Start Time','Delivery End Time'))\n",
    "        rdc_errors = pd.DataFrame(columns=('Store Number','DP Name','Ship','Delivery','Status','Errors'))\n",
    "\n",
    "        \n",
    "### Export csv - ignore the errors file if there are none, but okay to export empty ODS and MCL\n",
    "if rdc_errors.empty:\n",
    "    pass\n",
    "else:\n",
    "    rdc_errors.to_csv('%s\\\\ODS_Errors_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %uploadpath, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "mcl_output.to_csv('%s\\\\MCL_upload_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %uploadpath, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "rdc_output.to_csv('%s\\\\ODS_upload_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %uploadpath, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***### Missing CDC ODS Report***\n",
    "\n",
    "Export all stores missing CDC ODS with the following conditions:\n",
    "    \n",
    "    Proposed open-date in next 60 days\n",
    "    CDC Only\n",
    "    Company-Operated Only    \n",
    "    Exclude Virtual\n",
    "    Don't need to include Lunch DSDs\n",
    "    Has active project in Pipeline Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merge the store's assigned sourcing group to the sourcing rules to get complete list of sourcing\n",
    "missing_ods = pd.merge(src,src_rules, on='srcgrp', how='left')\n",
    "missing_ods.drop(columns={'prec'}, inplace=True)\n",
    "\n",
    "### Merge the sourcing to ODS\n",
    "missing_ods = pd.merge(missing_ods, ods, left_on=('str','dpnum'), right_on=('strnum','DPNum'), how='left')\n",
    "missing_ods.drop(columns={'strnum','cstnum','sitenum','DPName','DPNum'}, inplace=True)\n",
    "\n",
    "### Only check for CDCs but exclude virtuals\n",
    "missing_ods = missing_ods[(missing_ods['dptype'] == 'CDC') & (~missing_ods['dpname'].str.contains('Virtual'))]\n",
    "\n",
    "### Check for blank ODS and treat as missing\n",
    "missing_ods['Freq'].replace(0,np.nan, inplace=True)\n",
    "\n",
    "### Select only where ODS is missing\n",
    "missing_ods = missing_ods[(missing_ods['Freq'].isnull())]\n",
    "\n",
    "### Flatten down the NS file to just one line per store with pertinent info\n",
    "ns_flatten = ns[['Store Number','X-dock CDC  (Proposed)','Bakery Temperature (Proposed)','Store Name',\\\n",
    "                 'Ship-to Address 1','Address 2','City','State/Province','Zip',\\\n",
    "                 'Country','BU','Proposed Open Date','Proposed Open Delta','Project Status']].copy()\n",
    "### Just make this column uniform, it was already filtered for only \"active\" statuses\n",
    "ns_flatten['Project Status'].where(ns_flatten['Project Status'].isnull(),'Active',inplace=True)\n",
    "### Drop duplicate lines so that we only return one line per store and ignore those w/o and active project\n",
    "ns_flatten.drop_duplicates(keep='first',inplace=True)\n",
    "ns_flatten = ns_flatten[(ns_flatten['Project Status'].notnull())]\n",
    "\n",
    "\n",
    "### Merge the missing ODS to the NS info\n",
    "missing_ods = pd.merge(missing_ods,ns_flatten, left_on='str', right_on='Store Number', how='left')\n",
    "\n",
    "### Filter to stores w/ proposed-open-date within next 60-days and sort\n",
    "today = pd.datetime.today().date()\n",
    "missing_range = today + timedelta(days=missingodsdays)\n",
    "missing_ods = missing_ods[(missing_ods['Proposed Open Date'] <= missing_range)]\n",
    "missing_ods.sort_values(by=['Proposed Open Date'], inplace=True)\n",
    "\n",
    "### Add some misc columns\n",
    "missing_ods['Add Delete Change'] = 'Add'\n",
    "missing_ods['Copy Week 1 to Week 2?'] = np.nan\n",
    "missing_ods['Schedule Type'] = 'Standard'\n",
    "missing_ods['blank1'] = np.nan\n",
    "missing_ods['blank2'] = np.nan\n",
    "missing_ods['blank3'] = np.nan\n",
    "missing_ods['blank4'] = np.nan\n",
    "missing_ods['blank5'] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "### Create a nicely formatted dataframe for output\n",
    "missing_output = missing_ods[['str','custnum','site','Store Name','Ship-to Address 1','Address 2',\\\n",
    "                             'City','State/Province','Zip','Country',\\\n",
    "                             'dpname','Add Delete Change','Copy Week 1 to Week 2?','Schedule Type', 'Proposed Open Date',\\\n",
    "                              'srcgrp','X-dock CDC  (Proposed)','blank1','blank2','blank3','blank4','blank5','Bakery Temperature (Proposed)']]\n",
    "\n",
    "### Save CSV to O: drive\n",
    "missing_output.to_csv('%sMissing_CDC_ODS_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %reportspath, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***### Reports***\n",
    "\n",
    "***\"Prosposed Open Date\" delta*** - check for large negative values (i.e. opening has been moved up)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the \"Prosposed Open Date\" delta report to the O: drive\n",
    "\n",
    "#Remove any row where there is not currently a value and eliminate duplicates\n",
    "#in either 'old' or 'new' column i.e. we only want changes, not new or old stores\n",
    "pdelta.dropna(axis=0, how='any', inplace=True)\n",
    "pdelta.drop_duplicates(inplace=True)\n",
    "\n",
    "#sort by the delta value - ascending, negative value means\n",
    "#opening has been moved up\n",
    "pdelta.sort_values(by=['Proposed Open Delta'], inplace=True)\n",
    "\n",
    "### Export the CSV to the O: drive\n",
    "pdelta.to_csv('%sProposed_Date_Changes_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %reportspath, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***### Exclusion List Reports***\n",
    "\n",
    "Checks if on exclusion list and flags if all ODS entered (filter out 0 freq)\n",
    "\n",
    "Checks if not on exclusion list but is missing ODS for 1 or more DPs (only include Company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Exclusion list Reports\n",
    "\n",
    "###Merge the store's assigned sourcing group to the sourcing rules to get complete list of sourcing\n",
    "check_ods = pd.merge(src,src_rules, on='srcgrp', how='left')\n",
    "check_ods.drop(columns={'custnum','site','prec','dptype'}, inplace=True)\n",
    "\n",
    "\n",
    "###Merge the sourcing to ODS\n",
    "check_ods = pd.merge(check_ods, ods, left_on=('str','dpnum'), right_on=('strnum','DPNum'), how='left')\n",
    "check_ods.drop(columns={'strnum','cstnum','sitenum','DPName','DPNum'}, inplace=True)\n",
    "\n",
    "###Exclude checking ODS for these DPs\n",
    "check_ods = check_ods[(check_ods['dpnum'] != 'CDC1099') & (check_ods['dpnum'] != 'RDC1198')]\n",
    "\n",
    "# ###Convert any valid ODS to 'y' and convert blank and missing ODS to 'n'\n",
    "check_ods['Freq'].replace(0,np.nan, inplace=True)\n",
    "check_ods['Freq'].where(check_ods['Freq'].isnull(),'y',inplace=True)\n",
    "check_ods['Freq'].replace(np.nan,'n', inplace=True)\n",
    "\n",
    "# check = check_ods[(check_ods['str'] == 11244) | (check_ods['str'] == 49793)]\n",
    "# print check\n",
    "\n",
    "###basically pivoting the counts of y and n per store   \n",
    "check_ods = check_ods.groupby(['str','Freq']).size()\n",
    "check_ods = check_ods.unstack()\n",
    "check_ods = check_ods.add_suffix('_Count').reset_index()\n",
    "\n",
    "### Join the exclusion list to the dataframe\n",
    "check_ods = pd.merge(check_ods, lm_exclu, left_on='str', right_on='strx', how='left' )\n",
    "\n",
    "\n",
    "###Build a list of stores to remove - i.e. they are on exclusion list and have an n_count of zero\n",
    "### n_count of zero means there are no DPs where we do not have ODS\n",
    "excl_remove = check_ods[(check_ods['strx'].notnull()) & (check_ods['n_Count'].isnull())]\n",
    "\n",
    "\n",
    "### Build a list of stores that need to be added to the exclusion list\n",
    "### i.e. they are not currently on the list and they have DPs where ODS is missing\n",
    "### missing means anything where n_Count is not null\n",
    "excl_add = check_ods[(check_ods['strx'].isnull()) & (check_ods['n_Count'].notnull())]\n",
    "\n",
    "### Join to the MMS store list so that we can sort out LS stores and add 'proposed open date'\n",
    "excl_add = pd.merge(excl_add, mms_str, left_on='str', right_on='strnum')\n",
    "excl_add.drop(columns={'strnum','STRNAM','ADDRESS_LINE_1','ADDRESS_LINE_2','CITY','State/Prov','POSTAL_CODE','COUNTRY_CODE','LATITUDE','LONGITUDE','Legal Entity'}, inplace=True)\n",
    "### Only include Company-Operated\n",
    "excl_add = excl_add[excl_add['OWNERSHIP_TYPE'] == 'CO']\n",
    "\n",
    "\n",
    "#Write the \"Remove from Exclusions\" report to the O: drive\n",
    "excl_remove.to_csv('%sExclusion_Remove_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %reportspath, index=False, encoding='utf-8')\n",
    "\n",
    "#Write the \"Add to Exclusions\" report to the O: drive\n",
    "excl_add.to_csv('%sExclusion_Add_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %reportspath, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***### Blank ODS report***\n",
    "\n",
    "Checks for blank ODS (i.e. has 0 for frequency but ignore nulls). Ignore Detroit (CDC1099) because it should be blank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_ods = ods[(ods['Freq'] == 0) & (ods['DPNum'] <> 'CDC1099')]\n",
    "blank_ods = pd.merge(blank_ods,lm_exclu,left_on='strnum', right_on='strx', how='left')\n",
    "\n",
    "blank_ods['strx'].where(blank_ods['strx'].isnull(), 'y', inplace=True)\n",
    "blank_ods.rename(columns={'strx':'On Exclusion List?'}, inplace=True)\n",
    "\n",
    "blank_ods.to_csv('%sBlank_ODS_{}.csv'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) %reportspath, index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####***Combine, consolidate and update*** the different data sources into a final file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Writing the data to ***EXCEL*** on O: Drive\n",
    "\n",
    "Writing as .csv seems cleaner, but the file size is much bigger than using Excelwriter for an .xlsx\n",
    "\n",
    "First cell: Create an XLSX combining ***Src, ODS and Account Numbers***\n",
    "\n",
    "Second cell: Create an XLSX with a merged ***MMS Store List and Lifecycle***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('%sOracle_Store_Data_{}.xlsx'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) % datapath)\n",
    "src.to_excel(writer,sheet_name='Src',columns=None, index=False)\n",
    "ods.to_excel(writer,sheet_name='ODS',columns=None, index=False)\n",
    "stroracle.to_excel(writer,sheet_name='Account',columns=None, index=False)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms_combined = pd.merge(mms_str,str_life, how='left', on='strnum')\n",
    "\n",
    "writer = pd.ExcelWriter('%sMMS_Storelist_{}.xlsx'.format(pd.datetime.today().strftime('%y%m%d-%H%M%S')) % datapath)\n",
    "mms_combined.to_excel(writer,sheet_name='MMS',columns=None, index=False)\n",
    "lm_exclu.to_excel(writer,sheet_name='Exclusion List',columns=None, index=False)\n",
    "\n",
    "writer.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
